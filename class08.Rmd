---
output:
  html_document: default
  pdf_document: default
---
```{r}
 tmp <- c(rnorm(30,-3), rnorm(30,3))
 x <- cbind(x=tmp, y=rev(tmp))
 plot(x)
 kmeans(x, centers=2, nstart = 20)
```

```{r}
 km <- kmeans(x, centers=2, nstart = 20)
```
Q. How many points are in each cluster?
  #30
Q. What ‘component’ of your result object details
 - cluster size?
    #size
 - cluster assignment/membership?
    #
 - cluster center?
    #
```{r}
km$size
```
#CLUSTER MEMBERSHIP assignment vector. (i.e. which cluster group)
```{r}
km$cluster
```
#    
```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```
```{r}
km$totss
```
    
#Hierarchical clustering is BEST USED when we don't know what the K value should be. 
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
```{r}
dist_matrix <- dist(x)
```


# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)

```{r}
hc <- hclust(d = dist_matrix)
```

# the print method is not so useful here
```{r}
hc
```

Call:
hclust(d = dist_matrix)
Cluster method : complete
Distance : euclidean
Number of objects: 60

# Our input is a distance matrix from the dist()
# function. Lets make sure we understand it first
dist_matrix <- dist(x)
dim(dist_matrix)
NULL
```{r}
dim(dist_matrix)
```

View( as.matrix(dist_matrix) )
dim(x)

```{r}
dim(x)
```

dim( as.matrix(dist_matrix) )
```{r}
dim( as.matrix(dist_matrix) )
```


# Note. symmetrical pairwise distance matrix
```{r}
plot(hc)
```
#notice that the numbers on the left side of the half are numbered #1-30; the other half is 31-60

```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, h=6) # Cut by height h

```


```{r}
cutree(hc, k=2 ) # Cut into k grps
```

#linking clusters in hierarchical clustering

```{r}
# Using different hierarchical clustering methods
d <- dist_matrix
hc.complete <- hclust(d, method="complete")
plot(hc.complete)

hc.average <- hclust(d, method="average")
plot(hc.average)

hc.single <- hclust(d, method="single")
plot(hc.single)

```



# Step 1. Generate some example data for clustering
```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```


# Step 2. Plot the data without clustering
```{r}
plot(x)
```

# Step 3. Generate colors for known clusters

# (just so we can compare to hclust results)
```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```


Q. Use the dist(), hclust(), plot() and cutree()
 functions to return 2 and 3 clusters
Q. How does this compare to your known 'col' groups?

```{r}
d < dist(x)
hc <- hclust(d)
plot(hc)
```



```{r}
plot(hc)
abline(h=2, col="red")
abline(h=2.5, col="blue")
```
# for hierarchical clustering, you need the dist.matrix and then  you would use the hierarchical matrix and then plot. 

```{r}
gp2 <- cutree(hc, k=2)
gp3 <- cutree(hc, k=3)
```

```{r}
plot(x, col=gp3)
```
# Kmeans( x, centers= 2, Nstart= 20) center represents the number of clusters, and Nstart represents the amount of iterations 
# Hclust(Dist(x))


# principal component analysis (PCA)
```{r}
#prcomp()
```


```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1)
head(mydata) 
```
#NOTE: prcomp() expects the samples to be rows and
genes to be columns so we need to first transpose the
matrix with the t() function!


```{r}
## lets do PCA
pca <- prcomp(t(mydata), scale=TRUE) 
pca
```

```{r}
attributes(pca) 
```

```{r}
## A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2]) 
```

```{r}
## Variance captured per PC
pca.var <- pca$sdev^2 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per
```
```{r}
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```



```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"


plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 

## Click to identify which sample is which
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata)) 
```

```{r}
#ncol(UK_foods_1_)
#nrow(UK_foods_1_)
```


```{r}
#x <- read.csv("data/UK_foods.csv")

```







```{r}
#head(UK_foods_1_)
#UK_foods_1_[,-1]
#x <- UK_foods_1_
```

```{r}
nrow(x)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
```{r}
pairs(x, col=rainbow(10), pch=16)

```
```{r}


```
```{r}

# Plot PC1 vs PC2
#x <- UK_foods_1_
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500)) 
text(pca$x[,1], pca$x[,2], colnames(x))
```


### this will help us determine how the original variables (dimensions) contribute to our new PC's
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

### kmeans(x, center=2, nstart=20)
hclust(dist(x))
PRCOMP(t(x))

